<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr">
    <head>
    	<link href="style.css" rel="stylesheet" type="text/css"/>
    	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
    	<link rel="shortcut icon" type="image/x-icon" href="images/Image1.ico" />
	    <title>Interpr&eacute;ation des mouvements de la main</title>
	    <style>
	    @import url('https://fonts.googleapis.com/css?family=Lato:100,300,400&display=swap');
	</style>
	    
		<!--[if lte IE 8]>
			<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
		<![endif]-->

    </head>

    <body>
	    <a id="_top"></a>
		<div id="global">
			<header>
				<ul>
					<li><a href="index.html">Accueil</a></li>
					<li id="current"><a href="#">Projet</a></li>
					<li><a href="galerie.html">Galerie</a></li>
					<li><a href="rapports.html">Rapports</a></li>
					<div class="underbar"></div>
				</ul>
				<hr />
			</header>
		    
			<div id="contenu">
				<div id="introduction">
					<div id="sommaire">
						<h4>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SOMMAIRE</h4>
						<ol>
							<li><a href="#_partie_1">&Eacute;tat de l'art</a></li>
							<li><a href="#_partie_2">D&eacute;tection des mains</a></li>
							<li><a href="#_partie_3">Filtrage de l'image de main</a></li>
							<li><a href="#_partie_4">D&eacute;tection des mouvements</a></li>
							<li><a href="#_partie_5">R&eacute;sultats</a></li>
						</ol>
					</div>
					<div>
						<p>
						Cette page vise à présenter le travail réalisé pendant le stage. L'algorithme dévellopé est divisable en trois grands sujets. Chaque sujet est l'une des parties de cette page.   
						</p>
						<p> Ce projet, initié par la cellule innovation d'Aubay France, vise à détecter et comprendre les gestes et mouvements de la main. Les enjeux du projets résident dans la crétation d'une intelligence artificielle capable de détecter ces gestes et mouvements ainsi que dans le traitement d'images pour binariser la main. 
						</p>
					</div>
				</div>
				<span class="clear"></span>
				<section>
					<a id="_partie_1"></a>
					<h1 class="titre">1. &Eacute;tat de l'art <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
					<p>

					La première étape du travail à consisté en l'assimilation et la compréhension des travaux précedemment effectué sur le sujet. 
					</p>
					Le projet existe depuis 2017 et certaines idées ont étées reprise pour la version de 2019. 
					<ul>
						<li>Binarisation de la main</li>
						<li>Détection des doigts via cercle inscrit du contour de la main </li>
						<li>Détection d'un squelette de la main</li>
					</ul>
					<p>
						Différentes recherches permettent de faire un état des projets et technolgies pouvant être utile au projet 
					</p>
					Pour la partie binarisation de la main :
					<ul>
						<li>Espace de couleur LAB</li>
						<li>Espace de couleur YCbCr</li>
						<li>Détection d'arriere plan par mélange de gaussienes</li>
						<li>Tracking de main</li>
					</ul>
					<p>


					</p>
					Pour la partie intelligence artificielle :
					<ul>
						<li>Réseau de neurones récurrents</li>
						<li>Séparation de la détection de la main et des mouvements</li>
						<li>Modèle <abbr title="You Only Look Once">YOLO</abbr> </li>
					</ul>
					<p>

					</p>
				</section>
				<section>
					<a id="_partie_3"></a>
					<h1 class="titre">2. Détection des mains <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
					<p>
					Pour détecter les mains le projet s'appuie sur le modèle YOLO pour You Only Look Once. Ce modèle permet de trouver le ou les objets pour lesquel s il à été entrainé avec une précision très honorable et une vitesse bien supérieures aux autres types de réseaux actuellement utilisés. 
					</p>
					<p>
					Après avoir constitué une imposante base de données il est possible d'entrainer le modèle pour reconnaitre les mains. Les images ci-dessous montrent l'evolution de la précision du modèle (en rouge) et le fonctoin de coût du modèle (en bleu) au cours du temps ainsi qu'un grand nombre de mains détéctées de manière fluide par le modèle. 
					</p>
					<p id="zonespic">
						<img src="images/chart.png" alt="Aprentisssage YOLO"/>
						<img src="images/pleinsdemains.png" alt="pleins de mains détéctées"/>
					</p>
					<p>
					Le modèle atteint une précision de 89% avec la métrique <abbr title="mean Average Precision">mAP</abbr> pour un <abbr title="Intersection over Union">IoU</abbr> correct si superieur à 50%. 
					</p>


				</section>
				
				<section>
					<a id="_partie_4"></a>
					<h1 class="titre">3. Filtrage de l'image de main <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
					<p>
					Une fois la ou les mains localisées il faut en extraire les points caractéristiques: centre, nombre de doigts ... Pour cela, l'algorithme effectue une série de traitement décrits ci-dessous.
					</p>

					<h4>Changement d'espace de couleur</h4>
					<p>
					La première étape est la binarisation de la main. Pour efectuer celle-ci le programme commence par convertir l'image dans l'espace de couleurs LAB. La photo ci-dessous montre le passage d'une main dans l'espave LAB. Ensuite, le seuillage est effectué par la méthode d'Otsu et permet d'obtenir une binarisation fidèle de la main
					</p>
										<p id="zonespic">
						<img src="images/m1.png" alt="Conversion en LAB"/>
					</p>

					<h4>Extraction du contour de la main</h4>
					<p>
					L'extraction des murs est un peu plus d&eacute;licate. Lorsque la Kinect prend une photo de son environnement, un fichier XML contenant la position et l'orientation du robot est cr&eacute;&eacute; et lui est associ&eacute;. Gr&acirc;ce &agrave;  ce fichier, les points du nuage peuvent-&ecirc;tre repositionn&eacute;s dans un rep&egrave;re "carte" qui peut-&ecirc;tre associ&eacute; &agrave;  celui de la carte g&eacute;n&eacute;r&eacute;e en 2 dimensions par le LIDAR. Une fois ce recalage effectu&eacute;, chaque point 3D va &ecirc;tre &eacute;tudi&eacute; pour d&eacute;terminer s'il est sur un mur ou non gr&acirc;ce &agrave;  la carte. Ensuite, comme pour les sols, les points conserv&eacute;s servent &agrave;  g&eacute;n&eacute;rer une vue 2D. Cette derni&egrave;re sera alors analys&eacute;e pour trouver les textures qui la compose.
					</p>
										<p id="zonespic">
						<img src="images/Hull.png" alt="Contour de Hull"/>
						<img src="images/mdifi.png" alt="Main binarisée"/>
					</p>

					<h4>Detection du squeltte et des doigts levés</h4>
					<p>
					Une fois que la vue d'ensemble est recr&eacute;&eacute; en 2D, on la d&eacute;coupe en vignette de 80 par 80 pixels. On a ainsi 8*6 vignettes par capture Kinect (qui font 640*480 pixels). On a choisi 80 puisque c'est un diviseur commun de 640 et 480 et il repr&eacute;sente une taille suffisamment grande pour obtenir des informations et suffisamment petite pour ne pas risquer d'avoir de nombreuses textures en m&ecirc;me temps dans la vignette.
					Afin de pouvoir conclure sur la texture, plusieurs param&egrave;tres sont observ&eacute;s :
					</p>
					<ul>
						<li>La couleur moyenne (3 dimensions)</li>
						<li>La dispersion de la couleur (3 dimensions)</li>
						<li>La couleur moyenne du Local Binary Pattern (LBP) (3 dimensions)</li>
						<li>La dispersion de la couleur moyenne du LBP (3 dimensions)</li>
					</ul>
					<p>
					Cet ensemble de param&egrave;tres forme un espace &agrave;  12 dimensions. Cet espace est s&eacute;par&eacute; en zones gr&acirc;ce &agrave;  une base de donn&eacute;es d'apprentissage. Ensuite, on observe de quelle zone s'approche le plus notre &eacute;chantillon pour conclure sur son type.
					</p>




				</section>
				
				<section>
					<a id="_partie_5"></a>
					<h1 class="titre">4. Détection des mouvements <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
					<p>
						Pour conclure ces d&eacute;veloppements, une mise en application directe a &eacute;t&eacute; faite durant la semaine du 4 au au 8 juin 2012 pour le d&eacute;fi Carotte. Durant cette semaine, toute l'&eacute;quipe s'est r&eacute;uni sur le site de la comp&eacute;tition &agrave;  Bourges pour affronter nos comp&eacute;titeurs (quatre autres consortiums liant grandes &eacute;coles et entreprises). Certains membres de l'&eacute;quipe ont ainsi pu rencontrer les ing&eacute;nieurs, doctorants et enseignants du LORIA, venant de Nancy. Auparavant, une semaine de test a &eacute;t&eacute; effectu&eacute;e afin de peaufiner les diff&eacute;rentes t&acirc;ches, s'assurer que les robots fonctionnent correctement m&eacute;caniquement (test unitaire) et tous ensemble (test collectif, simulations de missions r&eacute;elles). Hormis un petit accident (une chute d'un robot dans les escaliers), les tests &eacute;taient dans l'ensemble concluant voire tr&egrave;s prometteur.</p>

					</p>
					<p id="zonespic">
					<img src="images/mvt.png" alt="Conversion d'un mouvement en image"/>
					</p>
					<p>
						Durant la semaine de comp&eacute;tition, ma mission a quelque peu chang&eacute;e. Ayant effectu&eacute; beaucoup de manipulation sur les robots (missions individuels et collectives), je suis devenu responsable du d&eacute;marrage des missions et de la restitution des donn&eacute;es. Cette t&acirc;che consiste &agrave;  la pr&eacute;paration logiciel des robots sur la ligne de d&eacute;part (V&eacute;rification de la connexion des robots au r&eacute;seau Wi-Fi, lancement d'une proc&eacute;dure logiciel de v&eacute;rification du mat&eacute;riel, d&eacute;marrage du protocole d'execution des missions) puis a ex&eacute;cuter les restitutions de donn&eacute;es lors du retour des robots en fin d'exploration. Pour les phases finales, ce dernier point &eacute;tait effectu&eacute; par un membre du jury pour assurer aucune triche. Je n'avais donc qu'&agrave;  m'assurer qu'il cliquait aux bons endroits (bien que les logiciels cr&eacute;&eacute;s pour l'occasion ne donnait pas beaucoupe de place pour une erreur).
					</p>
					<p>
						Comme expliqu&eacute; dans l'introduction, cette semaine fut tr&egrave;s concluante pour l'&eacute;quipe puisque nous avons termin&eacute; premier du challenge. Cette victoire devrait ainsi augmenter la visibilit&eacute; des laboratoires du LISA et du LORIA aupr&egrave;s de grands organismes de recherches. Le concours et les trois ann&eacute;es de recherches ont aussi &eacute;t&eacute; l'occasion d'&eacute;crire des publications scientifiques et de d&eacute;poser un brevet.
					</p>
					<p id="zonespic">
					<img src="images/acc_png.png" alt="Evolution de la precision et de la fonciton loss en fonction du temps"/>
					</p>
				</section>
				
				<section>
					<a id="_partie_6"></a>
					<h1 class="titre">5.  Résultats <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
				
					<p>
					En combinant la détection de gestes et de mouvements, une multitude d'actions peuvent être interprétes. Ces actions sont ensuite converties en actions faites par l'orfinateur ou l'objet lié à notre programme. 
					</p>
					<p>
					Ce travail se conclut par la création de deux interfaces de travail utilisant l'interprétation des gestes et mouvements pour interagir avec l'ordinateur. Une version de démonstration présentée dans la galerie du site et une version utilisateur, plus petite et toujours à l'ecran permettant une fluidité dans son utilisation. 
					</p>
				</section>
			</div>

			<footer>
				<hr />
				<ul>
					<li><a href="http://www.univ-angers.fr/"><img src="images/UnivAngers.png" title="Université d'Angers" alt="Logo de l'université d'Angers"/></a></li>
					<li><a href="https://www.aubay.com/"><img src="images/aubay.jpg" title="Aubay" alt="Logo d'Aubay"/></a></li>
					<li><a href="http://www.istia.univ-angers.fr/LISA/"><img src="images/ensta.jpg" title="Ensta Bretagne" alt="Logo dd l'ensta"/></a></li>
				</ul>
			</footer>
		</div>
    </body>
</html>
