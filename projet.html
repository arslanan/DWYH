<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr">
    <head>
    	<link href="style.css" rel="stylesheet" type="text/css"/>
    	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
    	<link rel="shortcut icon" type="image/x-icon" href="images/Image1.ico" />
	    <title>Interpr&eacute;ation des mouvements de la main</title>
	    <style>
	    @import url('https://fonts.googleapis.com/css?family=Lato:100,300,400&display=swap');
	</style>
	    
		<!--[if lte IE 8]>
			<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
		<![endif]-->

    </head>

    <body>
	    <a id="_top"></a>
		<div id="global">
			<header>
				<ul>
					<li><a href="index.html">Accueil</a></li>
					<li id="current"><a href="#">Projet</a></li>
					<li><a href="galerie.html">Galerie</a></li>
					<li><a href="rapports.html">Rapports</a></li>
					<div class="underbar"></div>
				</ul>
				<hr />
			</header>
		    
			<div id="contenu">
				<div id="introduction">
					<div id="sommaire">
						<h4>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SOMMAIRE</h4>
						<ol>
							<li><a href="#_partie_1">&Eacute;tat de l'art</a></li>
							<li><a href="#_partie_2">D&eacute;tection des mains</a></li>
							<li><a href="#_partie_3">Filtrage de l'image de main</a></li>
							<li><a href="#_partie_4">D&eacute;tection des mouvements</a></li>
							<li><a href="#_partie_5">R&eacute;sultats</a></li>
						</ol>
					</div>
					<div>
						<p>
						Cette partie va maintenant pr&eacute;senter les diff&eacute;rents travaux qui ont &eacute;t&eacute; effectu&eacute; pendant le stage. Chaque grande &eacute;tape sera repr&eacute;sent&eacute;e dans une section, mais cela ne signifie pas pour autant qu'elle est ind&eacute;pendante des autres.
						</p>
						<p> L'aboutissement de ce stage est en partie repr&eacute;sent&eacute; par le d&eacute;fi <abbr title="CArtographie par ROboT d'un TErritoire">Carotte</abbr> qui s'est d&eacute;roul&eacute; du 4 au 8 juin 2012. Le but de ce d&eacute;fi est de r&eacute;aliser la cartographie d'un espace ferm&eacute; de 120m&sup2; et d'identifier et localiser des objets dans cet espace. Ce concours est g&eacute;r&eacute; par la <abbr title="Direction G&eacute;n&eacute;rale de l'Armement">DGA</abbr> et l'<abbr title="Agence Nationale pour la Recherche">ANR</abbr>. Il se d&eacute;roulait sur trois ans. Le travail au sein du laboratoire a &eacute;t&eacute; r&eacute;parti entre deux laboratoires, le <abbr title="Laboratoire d'Ing&eacute;nierie des Syst&egrave;mes Automatis&eacute;s">LISA</abbr> &agrave;  Angers et le <abbr title="Laboratoire LOrrain de Recherche en Informatique et ses Applications">LORIA</abbr> &agrave;  Nancy. Pour cette derni&egrave;re ann&eacute;e, l'&eacute;quipe est fi&egrave;re d'annoncer une premi&egrave;re place au concours !
						</p>
					</div>
				</div>
				<span class="clear"></span>
				<section>
					<a id="_partie_1"></a>
					<h1 class="titre">1. &Eacute;tat de l'art <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
					<p>
					Dans un premier temps, une recherche a &eacute;t&eacute; faite afin de faire l'inventaire sur les techniques et algorithmes correspondant aux probl&egrave;mes de la reconnaissance de plan et d'objets. Cette recherche a permis d'identifier plusieurs solutions possibles qui sont list&eacute;es ci-dessous. En gras sont mis en &eacute;vidence les algorithmes ayant un lien partiel ou total avec la r&eacute;solution finale.
					</p>
					Reconnaissance de plans :
					<ul>
						<li><b>Transformation de Hough</b></li>
						<li><abbr title="RANdom SAmple Consensus">RANSAC</abbr></li>
						<li>Division - fusion</li>
						<li><b>Croissance de surfaces</b></li>
					</ul>
					Reconnaissance d'objets :
					<ul>
						<li>&agrave;€ partir d'images 2D</li>
						<li><b>D&eacute;tection de caract&eacute;ristiques</b></li>
						<li>Approche g&eacute;om&eacute;trique</li>
						<li><b>Utilisation de caract&eacute;ristiques cl&eacute;s</b></li>
					</ul>
					<p>
					Cette phase pr&eacute;c&eacute;dant le travail de d&eacute;veloppement s'est d&eacute;roul&eacute;e durant les mois de projets de la formation d'ing&eacute;nieur. Elle s'est conclut par un rapport et une soutenance mi-f&eacute;vrier 2012.
					</p>
				</section>
				
				
				<section>
					<a id="_partie_3"></a>
					<h1 class="titre">2. Détection des mains <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
					<p>
					Cette partie n'aura malheureusement abouti &agrave;  aucun r&eacute;sultat exploitable. En effet, ce travail a &eacute;t&eacute; fait dans un but d'exploration et sans promesse de r&eacute;ussite.<br />
					Un des premiers algorithmes de classification des objets &eacute;tait bas&eacute; sur une m&eacute;thode de regroupement en gaussienne des caract&eacute;ristiques des objets. Cette m&eacute;thode est g&eacute;n&eacute;ralisable &agrave;  d'autres buts et permet donc a priori de s&eacute;parer n'importe quel espace en clusters sous forme de gaussienne. Nous avons donc voulu l'utiliser pour r&eacute;ussir &agrave;  segmenter une image poss&egrave;dant plusieurs objets superpos&eacute;s afin de trouver des zones et donc de s&eacute;parer les objets "attach&eacute;s". Malheureusement, l'&eacute;tape de discretisation de l'image &eacute;tait tr&egrave;s consommatrice en m&eacute;moire et prenait ensuite &eacute;norm&eacute;ment de temps &agrave;  analyser. Cette m&eacute;thode a &eacute;t&eacute; essay&eacute;e pour travailler sur les couleurs (espace HSV, 3 dimensions) puis ensuite sur les couleurs ET une information spatiale (HS et XY, 4 dimensions). Quelques r&eacute;sultats montrant cette transformation sont illustr&eacute;s ci-dessous. Dans l'ordre nous avons l'image originale, le traitement en utilisant uniquement la couleur (HSV) puis le m&ecirc;me traitement en servant de la couleur (H et S) et l'espace (X et Y).
					</p>
					<p id="zonespic">
						<img src="images/zones.png" alt="image originale"/>
						<img src="images/zonesHSV.png" alt="zones HSV"/>
						<img src="images/zonesHSXY.png" alt="zones HS+XY"/>
					</p>
				</section>
				
				<section>
					<a id="_partie_4"></a>
					<h1 class="titre">3. Filtrage de l'image de main <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
					<p>
					Lors du dernier mois avant la comp&eacute;tition, il a &eacute;t&eacute; d&eacute;cid&eacute; de se pencher sur l'analyse des textures des sols et des murs afin de remporter des points suppl&eacute;mentaires (10% de la note finale sur cette &eacute;preuve). Pour cela, il fallait tout d'abord extraire les sols et les cloisons des nuages 3D afin de pouvoir reconstruire des images 2D avec uniquement ces zones.
					</p>
					<img id="texturespic" src="images/textures.png" alt="illustrations textures"/>
					<h4>Extraction des sols</h4>
					<p>
					La proc&eacute;dure de segmentation du sol est assez simple. Il nous suffit de regarder l'altitude des points du nuage et de ne conserver que ceux qui sont en dessous d'un certain seuil. Ensuite, une image 2D est reconstruite &agrave;  partir des points restants. Cette image est ensuite analys&eacute;e pour d&eacute;terminer les textures qui sont dessus.
					</p>
					<h4>Extraction des murs</h4>
					<p>
					L'extraction des murs est un peu plus d&eacute;licate. Lorsque la Kinect prend une photo de son environnement, un fichier XML contenant la position et l'orientation du robot est cr&eacute;&eacute; et lui est associ&eacute;. Gr&acirc;ce &agrave;  ce fichier, les points du nuage peuvent-&ecirc;tre repositionn&eacute;s dans un rep&egrave;re "carte" qui peut-&ecirc;tre associ&eacute; &agrave;  celui de la carte g&eacute;n&eacute;r&eacute;e en 2 dimensions par le LIDAR. Une fois ce recalage effectu&eacute;, chaque point 3D va &ecirc;tre &eacute;tudi&eacute; pour d&eacute;terminer s'il est sur un mur ou non gr&acirc;ce &agrave;  la carte. Ensuite, comme pour les sols, les points conserv&eacute;s servent &agrave;  g&eacute;n&eacute;rer une vue 2D. Cette derni&egrave;re sera alors analys&eacute;e pour trouver les textures qui la compose.
					</p>
					<h4>D&eacute;termination de la texture</h4>
					<p>
					Une fois que la vue d'ensemble est recr&eacute;&eacute; en 2D, on la d&eacute;coupe en vignette de 80 par 80 pixels. On a ainsi 8*6 vignettes par capture Kinect (qui font 640*480 pixels). On a choisi 80 puisque c'est un diviseur commun de 640 et 480 et il repr&eacute;sente une taille suffisamment grande pour obtenir des informations et suffisamment petite pour ne pas risquer d'avoir de nombreuses textures en m&ecirc;me temps dans la vignette.
					Afin de pouvoir conclure sur la texture, plusieurs param&egrave;tres sont observ&eacute;s :
					</p>
					<ul>
						<li>La couleur moyenne (3 dimensions)</li>
						<li>La dispersion de la couleur (3 dimensions)</li>
						<li>La couleur moyenne du Local Binary Pattern (LBP) (3 dimensions)</li>
						<li>La dispersion de la couleur moyenne du LBP (3 dimensions)</li>
					</ul>
					<p>
					Cet ensemble de param&egrave;tres forme un espace &agrave;  12 dimensions. Cet espace est s&eacute;par&eacute; en zones gr&acirc;ce &agrave;  une base de donn&eacute;es d'apprentissage. Ensuite, on observe de quelle zone s'approche le plus notre &eacute;chantillon pour conclure sur son type.
					</p>
				</section>
				
				<section>
					<a id="_partie_5"></a>
					<h1 class="titre">4. Détection des mouvements <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
					<p>
						Pour conclure ces d&eacute;veloppements, une mise en application directe a &eacute;t&eacute; faite durant la semaine du 4 au au 8 juin 2012 pour le d&eacute;fi Carotte. Durant cette semaine, toute l'&eacute;quipe s'est r&eacute;uni sur le site de la comp&eacute;tition &agrave;  Bourges pour affronter nos comp&eacute;titeurs (quatre autres consortiums liant grandes &eacute;coles et entreprises). Certains membres de l'&eacute;quipe ont ainsi pu rencontrer les ing&eacute;nieurs, doctorants et enseignants du LORIA, venant de Nancy. Auparavant, une semaine de test a &eacute;t&eacute; effectu&eacute;e afin de peaufiner les diff&eacute;rentes t&acirc;ches, s'assurer que les robots fonctionnent correctement m&eacute;caniquement (test unitaire) et tous ensemble (test collectif, simulations de missions r&eacute;elles). Hormis un petit accident (une chute d'un robot dans les escaliers), les tests &eacute;taient dans l'ensemble concluant voire tr&egrave;s prometteur.</p>
					<p>
						Durant la semaine de comp&eacute;tition, ma mission a quelque peu chang&eacute;e. Ayant effectu&eacute; beaucoup de manipulation sur les robots (missions individuels et collectives), je suis devenu responsable du d&eacute;marrage des missions et de la restitution des donn&eacute;es. Cette t&acirc;che consiste &agrave;  la pr&eacute;paration logiciel des robots sur la ligne de d&eacute;part (V&eacute;rification de la connexion des robots au r&eacute;seau Wi-Fi, lancement d'une proc&eacute;dure logiciel de v&eacute;rification du mat&eacute;riel, d&eacute;marrage du protocole d'execution des missions) puis a ex&eacute;cuter les restitutions de donn&eacute;es lors du retour des robots en fin d'exploration. Pour les phases finales, ce dernier point &eacute;tait effectu&eacute; par un membre du jury pour assurer aucune triche. Je n'avais donc qu'&agrave;  m'assurer qu'il cliquait aux bons endroits (bien que les logiciels cr&eacute;&eacute;s pour l'occasion ne donnait pas beaucoupe de place pour une erreur).
					</p>
					<p>
						Comme expliqu&eacute; dans l'introduction, cette semaine fut tr&egrave;s concluante pour l'&eacute;quipe puisque nous avons termin&eacute; premier du challenge. Cette victoire devrait ainsi augmenter la visibilit&eacute; des laboratoires du LISA et du LORIA aupr&egrave;s de grands organismes de recherches. Le concours et les trois ann&eacute;es de recherches ont aussi &eacute;t&eacute; l'occasion d'&eacute;crire des publications scientifiques et de d&eacute;poser un brevet.
					</p>
				</section>
				
				<section>
					<a id="_partie_6"></a>
					<h1 class="titre">5.  Résultats <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
				
					<p>
					En combinant la détection de gestes et de mouvements, une multitude d'actions peuvent être interprées. Ces actions sont ensuite converties en actions faites par l'orfinateur ou l'objet lié à notre programme. 

					Ce travail se conclut par la création de deux interfaces de travail utilisant l'interprétation des gestes et mouvements pour interagir avec l'ordinateur. Une version de démonstration présentée dans la galerie du site et une version utilisateur, plus petite et toujours à l'ecran permettant une fluidité dans son utilisation. 
					</p>
				</section>
			</div>

			<footer>
				<hr />
				<ul>
					<li><a href="http://www.univ-angers.fr/"><img src="images/UnivAngers.png" title="Université d'Angers" alt="Logo de l'université d'Angers"/></a></li>
					<li><a href="https://www.aubay.com/"><img src="images/aubay.jpg" title="Aubay" alt="Logo d'Aubay"/></a></li>
					<li><a href="http://www.istia.univ-angers.fr/LISA/"><img src="images/ensta.jpg" title="Ensta Bretagne" alt="Logo dd l'ensta"/></a></li>
				</ul>
			</footer>
		</div>
    </body>
</html>
