<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr">
    <head>
    	<link href="style.css" rel="stylesheet" type="text/css"/>
    	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
    	<link rel="shortcut icon" type="image/x-icon" href="images/Image1.ico" />
	    <title>Interpr&eacute;ation des mouvements de la main</title>
	    <style>
	    @import url('https://fonts.googleapis.com/css?family=Lato:100,300,400&display=swap');
	</style>
	    
		<!--[if lte IE 8]>
			<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
		<![endif]-->

    </head>

    <body>
	    <a id="_top"></a>
		<div id="global">
			<header>
				<ul>
					<li><a href="index.html">Accueil</a></li>
					<li id="current"><a href="#">Projet</a></li>
					<li><a href="galerie.html">Galerie</a></li>
					<li><a href="rapports.html">Rapports</a></li>
					<div class="underbar"></div>
				</ul>
				<hr />
			</header>
		    
			<div id="contenu">
				<div id="introduction">
					<div id="sommaire">
						<h4>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; SOMMAIRE</h4>
						<ol>
							<li><a href="#_partie_1">&Eacute;tat de l'art</a></li>
							<li><a href="#_partie_2">D&eacute;tection des mains</a></li>
							<li><a href="#_partie_3">Filtrage de l'image de main</a></li>
							<li><a href="#_partie_4">D&eacute;tection des mouvements</a></li>
							<li><a href="#_partie_5">R&eacute;sultats</a></li>
						</ol>
					</div>
					<div>
						<p>
						Cette page vise à présenter le travail réalisé pendant le stage. L'algorithme développé est divisible en trois grands sujets. Chaque sujet est l'une des parties de cette page.   
						</p>
						<p> Ce projet, initié par la cellule innovation d'Aubay France, vise à détecter et comprendre les gestes et mouvements de la main. Les enjeux du projets résident dans la création d'une intelligence artificielle capable de détecter ces gestes et mouvements ainsi que dans le traitement d'images pour binariser la main. 
						</p>
					</div>
				</div>
				<span class="clear"></span>
				<section>
					<a id="_partie_1"></a>
					<h1 class="titre">1. &Eacute;tat de l'art <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
					<p>

					La première étape du travail à consisté en l'assimilation et la compréhension des travaux précédemment effectué sur le sujet. 
					</p>
					
					<ul>
						<li>Binarisation de la main</li>
						<li>Détection des doigts via cercle inscrit du contour de la main </li>
						<li>Détection d'un squelette de la main</li>
					</ul>
					<p>
						Différentes recherches permettent de faire un état des projets et technologies pouvant être utile au projet 
					</p>
					Pour la partie binarisation de la main :
					<ul>
						<li>Espace de couleur LAB</li>
						<li>Espace de couleur YCbCr</li>
						<li>Détection d'arriere plan par mélange de gaussienes</li>
						<li>Tracking de main</li>
					</ul>
					<p>


					</p>
					Pour la partie intelligence artificielle :
					<ul>
						<li>Réseau de neurones récurrents</li>
						<li>Segmentation sémantique</li>
						<li>Séparation de la détection de la main et des mouvements</li>
						<li>Modèle <abbr title="You Only Look Once">YOLO</abbr> </li>
					</ul>
					<p>

					</p>
				</section>
				<section>
					<a id="_partie_3"></a>
					<h1 class="titre">2. Détection des mains <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
					<p>
					Pour détecter les mains, le projet s'appuie sur le modèle YOLO pour You Only Look Once. Ce modèle permet de trouver le ou les objets pour lesquels s il a été entraîné avec une précision très honorable et une vitesse bien supérieures aux autres types de réseaux actuellement utilisés. 
					</p>
					<p>
					Après avoir constitué une imposante base de données, il est possible d'entraîner le modèle pour reconnaître les mains. Les images ci-dessous montrent l'évolution de la précision du modèle (en rouge) et la fonction de coût du modèle (en bleu) au cours du temps ainsi qu'un grand nombre de mains détectées de manière fluide par le modèle. 
					</p>
					<p id="zonespic20">
						<img src="images/chart.png" alt="Aprentisssage YOLO"/>
					</p>
					<p id="zonespic10">
						<img src="images/pleinsdemains.png" alt="pleins de mains détéctées"/>
					</p>
					<p>
					Le modèle atteint une précision de 89% avec la métrique <abbr title="mean Average Precision">mAP</abbr> pour un <abbr title="Intersection over Union">IoU</abbr>correct si superieur à 50%. Cette précision est amplement suffisante pour détecter la majorité des mains. La fluidité à cette étape est de 30 images par seconde.
					</p>


				</section>
				
				<section>
					<a id="_partie_4"></a>
					<h1 class="titre">3. Filtrage de l'image de main <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
					<p>
					Une fois la ou les mains localisées il faut en extraire les points caractéristiques: centre, nombre de doigts ... Pour cela, l'algorithme effectue une série de traitement décrits ci-dessous.
					</p>

					<h4>Changement d'espace de couleur</h4>
					<p>
					La première étape est la binarisation de la main. Pour efectuer celle-ci le programme commence par convertir l'image dans l'espace de couleurs LAB. La photo ci-dessous montre le passage d'une main dans l'espave LAB. Ensuite, le seuillage est effectué par la méthode d'Otsu et permet d'obtenir une binarisation fidèle de la main
					</p>
										<p id="zonespic5">
						<img src="images/m1.png" alt="Conversion en LAB"/>
						<img src="images/mdifi.png" alt="Main binarisée"/>
					</p>

					<h4>Extraction du contour de la main</h4>
					<p>
					Une fois la main binarisée, il est possible d'extraire son contour. A l'aide des moments de cette image on obtient la position du centre de la main. En calculant ensuite le contour de Hull de ce contour de main (image ci-dessous) il est possible de trouver l'ensemble des doigts de la main. 
					</p>
						<p id="zonespic5">
						<img src="images/Hull.png" alt="Contour de Hull"/>

					</p>

					<h4>Detection du squeltte et des doigts levés</h4>
					<p>
					Une fois la position des doigts connue, il est possible de trouver un squelette de la main (figure ci-dessous). En interprétant plusieurs paramètres tels que l'angle entre les doigts et le nombre de doigts levés, il est possible de déterminer quel geste est effectuée. 
					</p>
					<p id="zonespic10">
					<img src="images/skelette.png" alt="Squelette de la main"/>
					</p>
					<p>
					Le programme est capable de reconnaitre 10 gestes différents. 
					</p>

				</section>
				
				<section>
					<a id="_partie_5"></a>
					<h1 class="titre">4. Détection des mouvements <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
					<p>
						Après être capable de détecter 10 gestes, l'outil est aussi capable de détecter cinq mouvements. Pour cela, un second modèle est entraîné. Le modèle est entraîné sur des images montrant l'évolution d la position de la main au cours du temps. La figure ci-dessous illustre les 5 mouvements reconnaissable ainsi qu'un mouvement parasite. 

					</p>
					<p id="zonespic10">
					<img src="images/mvt.png" alt="Conversion d'un mouvement en image"/>
					</p>
					<p>
						Le réseau a été confronté à du surraprentissage, après plusieurs révisions, il atteint un score de 95.5% sur le jeu de test. Le modèle détecte 100% des mouvements de la base d'entraînement. En détectant 5 mouvements et 10 gestes, il est possible de réaliser un grand nombre d'interactions. 
					</p>
					<p id="zonespic20">
					<img src="images/acc_png.png" alt="Evolution de la precision et de la fonciton loss en fonction du temps"/>
					</p>
				</section>
				
				<section>
					<a id="_partie_6"></a>
					<h1 class="titre">5.  Résultats <a href="#_top"><img src="images/top.png" alt="top"/></a></h1>
				
					<p>
					En combinant la détection de gestes et de mouvements, une multitude d'actions peuvent être interprétés. Ces actions sont ensuite converties en actions faites par l'ordinateur ou l'objet lié à notre programme. 
					</p>
					<p>
					Ce travail se conclut par la création de deux interfaces de travail utilisant l'interprétation des gestes et mouvements pour interagir avec l'ordinateur. Une version de démonstration présentée dans la galerie du site et une version utilisateur, plus petite et toujours à l'écran permettant une fluidité dans son utilisation. 
					</p>
				</section>
			</div>

			<footer>
				<hr />
				<ul>
					<li><a href="http://www.univ-angers.fr/"><img src="images/UnivAngers.png" title="Université d'Angers" alt="Logo de l'université d'Angers"/></a></li>
					<li><a href="https://www.aubay.com/"><img src="images/aubay.jpg" title="Aubay" alt="Logo d'Aubay"/></a></li>
					<li><a href="http://www.istia.univ-angers.fr/LISA/"><img src="images/ensta.jpg" title="Ensta Bretagne" alt="Logo dd l'ensta"/></a></li>
				</ul>
			</footer>
		</div>
    </body>
</html>
